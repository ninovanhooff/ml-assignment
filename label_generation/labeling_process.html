<!DOCTYPE html><html><head><meta charset="utf-8"><title>Labeling process</title><style></style></head><body id="preview">
<h2><a id="Get_a_list_of_robot_user_agents_from_0"></a>Get a list of robot user agents from</h2>
<p>[<a href="https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json">https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json</a>]</p>
<h2><a id="filter_the_json_on_user_agent_key_3"></a>filter the json on user agent key</h2>
<pre><code>jq -c 'map(with_entries(select(.key == (&quot;pattern&quot;))))'
</code></pre>
<p>==&gt; robot_patterns.json</p>
<h2><a id="get__the_user_agent_from_each_line_9"></a>get  the user agent from each line</h2>
<pre><code>grep ^(?!&quot;pattern&quot;).*$
</code></pre>
<p>==&gt; robot_patterns.txt</p>
<h2><a id="filter_to_leave_only_bot_names_present_in_data_fish_shell_15"></a>filter to leave only bot names present in data (fish shell)</h2>
<p>Input <code>data_user_agents.txt</code> is simply the use agent column of the given <code>clickdata.csv</code></p>
<ul>
<li>loop over each line</li>
<li>print occurrences robot patterns</li>
<li>count unique occurrences of robot patterns</li>
<li>sort by count</li>
</ul>
<pre><code>for line in (cat robot_patterns.txt); grep -oh $line data_user_agents.txt ; end | uniq -cd | sort -nr &gt;&gt; ./data_uniq
</code></pre>
<h2><a id="To_Tabdelimited_csv_27"></a>To Tab-delimited csv</h2>
<p>Replace space by tab</p>
<pre><code>cat ./data_uniq | sed 's/^\s*//' | sed 's/ /\t/g;s/ /,/' &gt; data_uniq.csv
</code></pre>
<h2><a id="Manually_label_the_bot_names_as_NHTSearch_or_NHTOther_34"></a>Manually label the bot names as NHT-Search or NHT-Other</h2>
<p>==&gt; labeled_web_crawlers.csv</p>
<table class="table table-striped table-bordered">
<thead>
<tr>
<th>Count</th>
<th>Name</th>
<th>Type</th>
<th>Info</th>
</tr>
</thead>
<tbody>
<tr>
<td>18123</td>
<td>Googlebot</td>
<td>NHT-search</td>
<td></td>
</tr>
<tr>
<td>1032</td>
<td>AdsBot-Google-Mobile</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>670</td>
<td>Mediapartners-Google</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>234</td>
<td>Sogou</td>
<td>NHT-search</td>
<td></td>
</tr>
<tr>
<td>208</td>
<td>bingbot</td>
<td>NHT-search</td>
<td></td>
</tr>
<tr>
<td>171</td>
<td>Applebot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>65</td>
<td>facebookexternalhit</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>48</td>
<td>WhatsApp</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>34</td>
<td>UptimeRobot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>33</td>
<td>pinterest</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>32</td>
<td>YandexBot</td>
<td>NHT-search</td>
<td></td>
</tr>
<tr>
<td>17</td>
<td>SemrushBot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Googlebot-Image</td>
<td>NHT-search</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>Gluten Free Crawler</td>
<td>NHT-other</td>
<td>Joke: searches for websites containing nog names of food products containing gluten</td>
</tr>
<tr>
<td>4</td>
<td>coccoc</td>
<td>NHT-other</td>
<td>Vietnamese browser</td>
</tr>
<tr>
<td>3</td>
<td>Twitterbot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Facebot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Discordbot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Cliqzbot</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Google Web Preview</td>
<td>NHT-other</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>AppEngine-Google</td>
<td>NHT-other</td>
<td></td>
</tr>
</tbody>
</table>

</body></html>